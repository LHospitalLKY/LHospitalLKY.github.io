<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Lei Kaiyu&#039; Trash Can</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lei Kaiyu&#039; Trash Can"><meta name="msapplication-TileImage" content="source/logo/GGBond.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Kaiyu&#039; Trash Can"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Avoir 20 ans. A rubbish Ph.D Candidate"><meta property="og:type" content="blog"><meta property="og:title" content="Lei Kaiyu&#039; Trash Can"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Lei Kaiyu&#039; Trash Can"><meta property="og:description" content="Avoir 20 ans. A rubbish Ph.D Candidate"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Lei Kaiyu"><meta property="article:tag" content="Asteroid"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lei Kaiyu' Trash Can","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lei Kaiyu"},"publisher":{"@type":"Organization","name":"Lei Kaiyu' Trash Can","logo":{"@type":"ImageObject","url":"http://example.com/logo/GGBond.jpg"}},"description":"Avoir 20 ans. A rubbish Ph.D Candidate"}</script><link rel="icon" href="/logo/GGBond.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/logo/GGBond.jpg" alt="Lei Kaiyu&#039; Trash Can" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/LHospitalLKY/lhospitalLKY.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><a class="image is-7by3" href="/2023/12/05/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/"><img class="fill" src="/images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/cover.png" alt="Online Shape Modeling of Resident Space Objects Through Implicit Scene Understanding"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-12-05T11:28:15.000Z" title="2023/12/5 19:28:15">2023-12-05</time>发表</span><span class="level-item"><time dateTime="2023-12-05T13:37:36.794Z" title="2023/12/5 21:37:36">2023-12-05</time>更新</span><span class="level-item">12 分钟读完 (大约1752个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/05/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/">Online Shape Modeling of Resident Space Objects Through Implicit Scene Understanding</a></p><div class="content"><p>本文为康奈尔大学Sibley School of Mechanical and Aerospace Engineering的Aneesh M. Heintz等人于2022年在Journal of Aerospace Information System上发表的文章，采用了一种view-synthesis网络来隐式表达小天体的形状，用于在小天体探测任务的抵近阶段（Approach）和初步探测阶段（Preliminary Survey）的resolved图像中来生成目标的形状。</p>
<p>本文主要提出了：</p>
<ol>
<li>一个包含两个深度网络的框架，包含一个view-synthesis network和一个graph convolutional network；</li>
<li>view-synthesis为系统提供环境理解能力，使用现有图像训练好该网络后，可以用来生成新的观测图像；</li>
<li>graph convolutional network则使用多视图图像集合来构建物体的三维图表示，从而生成物体的形状模型</li>
</ol>
<p>本文作者认为，经过仿真实验测试，本文提出的pipeline在模型精确性方面，有着与state-of-art methods相当的竞争力。此外，本文的pipeline能够在多环境任务中应用。最后，相比state-of-art methods，本文方法有着较高的计算效率。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>本文的训练和测试均采用仿真数据。本文首先设定一系列轨道，这些轨道分为两类：抵进轨道和Preliminary survey轨道（文中提到每一类轨道设定500条，这或许是为了验证本文Pipeline在不同环境下的性能？）。对于Preliminary survey来说，使用的轨道为简单球形轨道，每条轨道上选择40个采样点。对于抵近轨道，使用Keplerian椭球轨道，目标将位于轨道的交点，每条轨道选择100个采样点。在采样点，将采集对目标的成像和当前飞行器的位置和姿态，构成image-pose pair。这些采样点都要在距离目标100km以内，防止距离太远目标变成点目标。仿真中，观测的目标使用四个已知形状的天体：Bennu、Itokawa、Toutatis、Mithra，其中Mithra只有地基雷达的低分形状模型，因此在评估建模精度时，只使用前三个。</p>
<p>对每条轨道的观测中，随机选择m组image-pose对，用来构成”场景”(constitute a scene)，随机选择一副图像作为query pair（这里的qurey pair是view-synthesis网络中所需要的），如图一所示；<br><img src="/../images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/%E6%9F%90%E6%AE%B5%E8%BD%A8%E9%81%93%E7%9A%84%E5%9C%BA%E6%99%AF%E4%BB%AC.png"></p>
<p>这些图像的分辨率都是$64\times 64$，所有图像都通过Blender生成。</p>
<p>光照条件将采用随机的方式，以保证训练集能够覆盖不同的光照条件（这里的随机或许是一条oribit一个光照方向？）。网络随机选取40000个场景，即40000个image-pose pair来进行训练。飞行器和目标之间的距离虽然要在100km以内，但也不能太近，从而模仿探测任务的早期阶段。对位姿来说，由于位姿是网络的输入，因此需要通过其他方法来得到位姿，为了方便，本文将采用SfM来得到位姿，但也可以通过其他方法来实现（本文作者的另一篇文章中，提出了使用NeRF来进行位姿估计的方法）。</p>
<h2 id="Pipeline-Architecture"><a href="#Pipeline-Architecture" class="headerlink" title="Pipeline Architecture"></a>Pipeline Architecture</h2><p>本文pipeline的结构如下图所示：<br><img src="/../images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/pipeline.png"></p>
<p>Pipeline分为两个部分，首先是Scene-Representation部分，使用文献[^1]中提到的GQN网络（Generative Query Netword）。关于该网络的细节，需要去相应文献中阅读。该网络的作用是理解场景信息，其结果是从输入的Sence中，学习出场景的信息，并得到新的视角下的图像。换句话说，GQN网络的作用和NeRF是相同的，但这种VAE-based网络有着相对更小的网络模型，运算速度会快一些。经过取舍，本文最终选择了GQN来作为场景理解的主干网络。</p>
<p>GQN的输入是一个场景序列${X^c, P^c}$，称为Context，Qurey序列只输入一系列的位姿${P^q}_m$，最终将输出更多的image-pose pair（文中的图与公式有些对不上）。这些结果如下所示：<br><img src="/../images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/GQN%E8%BE%93%E5%87%BA.png"></p>
<p>原始的Context和GQN生成的Context，将作为Shape-Modeling Graph-convolution Network(GCN)的输入。该部分采用pixel2mesh网络[^2]的主干结构，但进行了多视角处理。同样，该网络的细节参考相应文献。</p>
<h2 id="实验与结论"><a href="#实验与结论" class="headerlink" title="实验与结论"></a>实验与结论</h2><p>本文进行了：1）对两个网络单独进行了测试；2）对整个pipeline进行了测试。</p>
<h3 id="单独测试"><a href="#单独测试" class="headerlink" title="单独测试"></a>单独测试</h3><h4 id="GQN结果"><a href="#GQN结果" class="headerlink" title="GQN结果"></a>GQN结果</h4><p>不太重要。</p>
<h4 id="GCN结果"><a href="#GCN结果" class="headerlink" title="GCN结果"></a>GCN结果</h4><p>实验结果表明，Preliminary survey轨道相比approach轨道，有着更好的表现，但并未给出具体的F1 score和模型的RMS对比。两段观测结果都表明，在不将GQN网络所生成的，在新视角下的图像加入GCN输入时，随着初始Context中包含的scene的增加，F Score将提升，特别是增加到25组scene时。</p>
<p>结果如下图所示：<br><img src="/../images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/preliminary%E8%BD%A8%E9%81%93%E5%8D%95%E7%8B%AC%E5%BB%BA%E6%A8%A1%E7%BB%93%E6%9E%9C.png" alt="&quot;Preliminary Survey轨道GCN单独测试结果&quot;"></p>
<p><img src="/../images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/approach%E8%BD%A8%E9%81%93%E5%8D%95%E7%8B%AC%E5%BB%BA%E6%A8%A1%E7%BB%93%E6%9E%9C.png" alt="&quot;Approach轨道GCN单独测试结果&quot;"></p>
<h3 id="Pipeline测试"><a href="#Pipeline测试" class="headerlink" title="Pipeline测试"></a>Pipeline测试</h3><p>对整个pipeline的测试有着相似的结果，Preliminary Survey轨道的三维建模精度高于Approach轨道，context中的scene越多，结果也越好。除此之外，本文还与现有的方法进行了对比，如下表所示：<br><img src="/../images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/%E4%B8%8E%E5%85%B6%E4%BD%99%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94.png" alt="alt"></p>
<p>从结果上看，在Preliminary段，本文的pipeline所构建的模型，相比SPC+LA的方法，在像素分辨率上毫无优势（40.91m v.s. 0.5m），但修正精度要高于SPC+LA（可以理解为像素误差）。在Approach段，有着同样的结论。</p>
<p>[^1]: Nguyen-Ha, P., Huynh, L., Rahtu, E., and Heikkilä, J., “Predicting Novel Views Using Generative Adversarial Query Network,” Scandi- navian Conference on Image Analysis, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), Vol. 11482 LNCS, Springer, 2019, pp. 16–27.</p>
<p>[^2]: Wang, N., Zhang, Y., Li, Z., Fu, Y., Yu, H., Liu, W., Xue, X., and Jiang, Y. G., “Pixel2Mesh: 3D Mesh Model Generation via Image Guided Deformation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 43, No. 10, Oct. 2021,<br>pp. 3600–3613.</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2023/11/16/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/"><img class="fill" src="/images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/title.png" alt="基于神经网络的光变曲线分类论文阅读(1)：A transfer learning approach to space debris classification using observational light curve data"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-11-16T02:44:23.000Z" title="2023/11/16 10:44:23">2023-11-16</time>发表</span><span class="level-item"><time dateTime="2023-11-16T14:48:03.968Z" title="2023/11/16 22:48:03">2023-11-16</time>更新</span><span class="level-item">1 小时读完 (大约7069个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/16/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/">基于神经网络的光变曲线分类论文阅读(1)：A transfer learning approach to space debris classification using observational light curve data</a></p><div class="content"><p>本文为J. Alworth[^1]等人与2021年发表在Acta Astronautica期刊上的一篇论文。主要介绍了一种基于迁移学习的空间（碎片）目标的光变曲线分类算法。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文所做的主要工作有：</p>
<ol>
<li>提出了一个适用于空间目标的1D卷积分类器，能够使用仿真&#x2F;真实光变数据对SO的形状进行分类</li>
<li>成功应用了迁移学习，提升了本文网络在真实光变上的分类精度。作者使用3D光线追踪软件来生成高真实度（high fidelity）的仿真数据，在仿真数据上对模型进行预训练，然后使用真实数据来对网络进行微调。</li>
<li>本文迁移学习的方式，对比纯使用真实数据训练的方式，有着更好的变现，该现象能够说明（indicate）迁移学习能够在真实光变数据缺失的情况下，更有效地使用深度学习技术。</li>
</ol>
<p>本文使用自行构造仿真数据集、MMT数据集上进行预训练，然后将模型迁移至EOS数据集并进行了测试，得到了不错的效果。（TODO: 补充实验结果）</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文在引言部分提到的背景为空间态势感知（Space Situational Awareness, SSA）。在该领域中，空间目标特别是空间碎片的特性是十分重要的信息。然而，绝大多数的空间碎片的关键特性是缺失的，这些特性包括目标的大小、形状、材质、旋转姿态等。这些信息对于目标轨道的精确预测是十分重要的。常驻空间目标（Resident Space Object, RSO）的特性通常使用地基观测手段来获取。</p>
<p>本文将目光聚焦于RSO的光变曲线。光度数据是地基观测中最容易获取的数据，亮度随着时间变换的曲线被称为光变曲线。光变曲线中，将RSO的大小、形状、材质等特性耦合进了目标的亮度变化中，对光变曲线的分析与反演能够对目标的特性做出一些估计。</p>
<p>然而，目前针对RSO的光变曲线反演方法，仅能在仿真数据集上，实现对简单形状与姿态的估计，这些方法尚未在真实光变数据上取得成功（论文作者观点）。</p>
<p>目标的特性与其亮度之间，是一个复杂的非线性关系。考虑到深度学习能够从数据中学习出这种复杂非线性关系，深度学习逐渐进入了光变反演研究者的视野。目前，在RSO光变反演中应用深度学习所面临的困难，主要集中在数据集方面。理论上来说，深度神经网络的规模越大，越能拟合出更加真实的关系，但响应的需要的训练数据量也越大。目前RSO光变曲线领域却十分缺少带有标注的数据（Labelled Dataset）。</p>
<p>为了绕开数据集，使得神经网络能够在数量不足的真实数据上达到较好的效果，本文采用了迁移学习的方法，使用仿真工具制造出大量的仿真光变数据集，在仿真数据集上进行预训练，接着使用真实数据集对网络进行微调（Fine-Tune），从而在insufficient的真实数据集上，得到一个效果还不错的网络模型。</p>
<p>本文的主要工作如下：</p>
<ol>
<li>设计并使用高保真度（high fidelity）的方法来仿真光变数据作为训练集（其实就是用blender的光线追踪引擎去仿真光变），用来提升神经网络的基准表现（boost the performance of neural network models for RSO characterisation based on the principles of transfer learning）；</li>
<li>设计了一个以CNN为骨干的网络，通过迁移学习，在仿真和真实数据集上得到很好的表现；</li>
<li>证明了迁移学习能够提升网络在真实光变数据上的表现，这种方法降低对well labelled数据的需求。</li>
</ol>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="1-光变仿真环境"><a href="#1-光变仿真环境" class="headerlink" title="1. 光变仿真环境"></a>1. 光变仿真环境</h3><p>本文的光变仿真工具力求生产出高保真度的仿真光变曲线（high fidelity）。仿真将使用给定的RSO几何与材质模型、轨道参数以及地基观测位置作为输入。根据这些信息来生成一系列的光变测量值，测量值的物理量为视星等。整个仿真过程如下图所示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/%E4%BB%BF%E7%9C%9F%E8%BF%87%E7%A8%8B.png"></p>
<p>整个仿真过程分为三大部分：</p>
<ol>
<li>初始化部分。初始化步骤中，设定目标ID、观测时长、采样频率、传感器参数配置，同时将卫星轨道信息输出为星历数据，获取观测几何。</li>
<li>3D渲染部分。该步骤输入目标的形状模型、初始姿态、自转轴与自转周期，然后根据星历数据，自动仿真出图像序列</li>
<li>光变提取部分。对渲染图像进行处理，将图像数据提取成目标的视星等，结合观测时间，构建光变数据。</li>
</ol>
<h4 id="Step-Initialization"><a href="#Step-Initialization" class="headerlink" title="Step: Initialization"></a>Step: Initialization</h4><p>所谓初始化步骤，其主要工作是根据目标的轨道信息、观测者位置信息、时间信息生成标准的星历文件，方便后续仿真时使用。</p>
<h4 id="Step-3D-Rendering"><a href="#Step-3D-Rendering" class="headerlink" title="Step: 3D Rendering"></a>Step: 3D Rendering</h4><p>3D渲染步骤中，使用RSO的几何模型与材质信息，设定初始姿态和自转轴、自转周期，根据上一步的星历信息，得到观测时刻的目标位置与姿态、太阳方位、观测位置，使用Blender的Cycle光线追踪引擎进行渲染，从而得到一系列的仿真图像。</p>
<h4 id="Step-Light-Curve-Extraction"><a href="#Step-Light-Curve-Extraction" class="headerlink" title="Step: Light Curve Extraction"></a>Step: Light Curve Extraction</h4><p>这一步骤的目的是为了从仿真得到的图像中计算出目标的视星等。计算视星等的基本原理公式为：<br>$$<br>m_1 &#x3D; m_2 + I_{m_1} - I_{m_2}<br>$$</p>
<p>其中，$m_2$为一个基准目标的视星等（例如恒星），该目标有一个已知的视星等，$I_{m_1}$与$I_{m_2}$则为从图像中提取出来的观测目标与基准目标的仪器星等。在理想传感器上，仪器星等与视星等之间是线性关系，因此通过二者的仪器星等的差异，能够直接从基准目标的视星等推算出目标星等。</p>
<p>类似的道理，在仿真过程中，上述公式中的基准目标，选择为一个Flat Facet Model。在仿真的时候，使用同样大小、材质的FFM，在同样的观测几何条件下进行仿真。最终的计算公式为：<br>$$<br>m_1 &#x3D; m_{ff} &#x3D; I_{m_b} - 2.5\log_{10}{\frac{B}{t_{exp} \cdot (\frac{d}{d_r} \cdot scale)^2}}<br>$$</p>
<p>其中，$m_{ff}$为flat plate在10000m出的绝对星等（这一部分如何计算的？），$I_{mb}$其相应的仪器星等，B是目标的所有像素的亮度和，$t_{exp}$是曝光时间，$d$是在Blender渲染中目标与观测者之间的距离，$d_r$是基准目标与观测者之间的距离，即10000m，scale是Blender渲染时，设置距离与实际距离之间的比例系数（因为如果按照实际距离，Blender仿真无法成像）。这个计算过程的问题就在于，如何得到$m_{ff}$与$I_{m_b}$。</p>
<h4 id="Post-Processing"><a href="#Post-Processing" class="headerlink" title="Post Processing"></a>Post Processing</h4><p>在得到仿真光变后，为了尽可能模拟真实数据的情况，需要进行后处理。仿真数据有着统一的采样长度和1s的均匀采样间隔，然而实际数据中，通常不会有统一的采样长度和均匀的采样间隔，会由于各种原因造成信号中断。本文采用如下的后处理，来模拟真实观测中的这种情况：</p>
<ol>
<li>使用高斯分布的随机数，随机选取数据段的起始与终止</li>
<li>采样频率上，模拟了EOS数据集，60%的数据设置为1Hz，30设置为0.5Hz，10设置为0.33Hz</li>
<li>同样使用高斯分布来随机去掉某些时刻的观测值，以模拟云层遮挡的情况</li>
</ol>
<h3 id="真实光变数据提取"><a href="#真实光变数据提取" class="headerlink" title="真实光变数据提取"></a>真实光变数据提取</h3><p>本文所使用的真实数据集包括EOS，该数据集是由位于澳大利亚两个观测站的六台望远镜所获取的数据，原始数据为巡天的图像数据，需要从图像中获取光变。</p>
<p>从图像中获取光变的如下图所示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/%E7%9C%9F%E5%AE%9E%E5%9B%BE%E5%83%8F%E5%85%89%E5%8F%98%E6%8F%90%E5%8F%96.png"></p>
<ol>
<li>使用平场、暗场图去掉noise；</li>
<li>从图像中区分天空背景与目标，同时使用快速傅里叶变换FFT来增强暗目标的识别；</li>
<li>使用PhotUtils从图像中寻找出恒星与目标，并计算出它们的视星等；</li>
<li>从图像中找出所要寻找的目标，剔除恒星，得到目标的视星等；</li>
<li>遍历所有时刻的图像，得到目标的光变曲线。</li>
</ol>
<p>视星等是通过仪器星等计算出来的，对于EOS数据集，六台望远镜仪器星等与视星等之间的转换关系是确定的，只要计算出仪器星等，即可得到对应的视星等。目标仪器星等计算公式为：<br>$$<br>I_m &#x3D; -2.5\log_{10}{\frac{N_{ap} - A_{ap}S_{sky}}{t_{exp}}}<br>$$</p>
<p>其中，$I_m$是目标的仪器星等，$N_{ap}$是目标所开的aperture中所有像素的灰度和，$A_{ap}$是aperture的面积，$S_{sky}$是星空背景的值（每像素的信号），$t-_{exp}$为曝光时间。</p>
<p>通过上述的过程，将EOS的图像数据转换为了对应目标的光变数据。</p>
<h3 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h3><p>本文以1D CNN为主干，构建了一个适用于利用光变数据进行目标分类的网络：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png"></p>
<p>整个网络分为两个Section，第一个Section包含两个卷积层，第二个Section包含3个全连接层，最后使用Softmax激活来实现目标分类。</p>
<p>1D CNN层包含了两个卷积层，每个卷积层之后都紧接着一个max pooling和一个bactch-norm。每个卷积层的stride都设置为3，不进行padding。</p>
<p>将卷积层的输出进行flat，作为输入传递给全连接层。本文网路共有3个全连接层，每个全连接层之后都有一个dropout和batch-norm。除最后一个全连接层外，卷积与全连接层的激活函数均为ReLU。最后一个全连接层使用softmax作为激活函数，实现最终的分类。</p>
<p>对于RSO类的物体，其亮度主要来源于反射太阳光，因此观测几何关系也会对目标的亮度产生影响。很多RSO自身都存在旋转，这是一段观测内引起亮度变化的主要因素。为了使网络能够将上述的因素加入拟合过程，除了星等外，以下三个数据也一并作为输入：</p>
<ol>
<li>太阳-目标-观测者相角</li>
<li>range（这里指的是目标的大小还是观测距离？不确定）</li>
<li>相对初始观测的时间（单位秒）</li>
</ol>
<p>本文最终所使用的真实数据集是EOS，该数据集每一段观测持续20分钟，因此输入光变数据的长度设置为1200，并基于该长度对数据进行截取与补零。</p>
<h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>本文的迁移学习过程如下图所示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0.png"></p>
<p>将预训练的模型进行迁移学习，其意义在于无需从头开始训练。在使用仿真数据进行预训练时，卷积Section、全连接Section部分的参数从随机设置开始，全部进行训练。当模型迁移到EOS数据上时，使用预训练的卷积Section作为Fine-Tuning的起始参数进行训练，而全连接Section的权重则重新初始化进行训练。由于数据量少，且卷积Section的参数从预训练好的模型中初始化，因此只需要较少的时间即可完成在EOS上的训练。</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>本文的第三部分对数据集、模型设置及超参数配置进行介绍。</p>
<h3 id="Blender仿真数据集"><a href="#Blender仿真数据集" class="headerlink" title="Blender仿真数据集"></a>Blender仿真数据集</h3><p>在本文中，使用Blender生成了4500条光变数据，这些数据来自于13类不同的卫星模型。每条光变的采样间隔为1s，每次仿真观测持续30min，即一条观测有1800个采样点。在仿真的初始化步骤中，生成不同观测相角、不同距离条件的TLE轨道文件，用来在仿真中确定太阳-目标-观测者之间的相对位置关系。观测目标的初始姿态、自转周期与自转轴将随机生成，自转周期在10s到300s之间随机生成，自转轴从三个主轴之间随机生成。自转特性的配置，将尽可能覆盖不同类型目标的自转状态。本文所创建的仿真数据中，尚未包含翻腾或自转很慢的目标，这部分内容作者声称将在后续工作中进行。</p>
<h3 id="EOS数据集"><a href="#EOS数据集" class="headerlink" title="EOS数据集"></a>EOS数据集</h3><p>截止作者成文时，EOS包含了9类configuration目标和22个unique目标的900多条光变数据。由于EOS原本的目标是跟踪火箭残骸这种large rotation objects，导致该数据集里约60%的目标都集中分布在3类不同的火箭结构体的类别中。这使得EOS成为了一个极度不平衡的数据集，甚至有3个分类，只包含了1个目标。</p>
<h3 id="MMT数据集"><a href="#MMT数据集" class="headerlink" title="MMT数据集"></a>MMT数据集</h3><p>Multichannel Monitoring Telescope(MMT)提供了另一个公开的RSO光变数据集。该数据集也将作为本文网络的训练与测试数据集。MMT数据集中的数据非常多，已经包含了10000个目标左右的数据量。本文中，只有拥有确定的自转周期，且超过500个独立观测的目标，才被选作数据集中的一员。</p>
<p>基于上述标准，有154个目标的40000条光变曲线被选入数据集中，这些目标被分到27个类别中。与EOS数据相同，这些数据也是极度unbalance的数据，最大的三个类别包含了75%的数据。为了本文的实验，在上述筛选的基础上，从8个数量较多的类别中，每一类选取500条光变，创造一个balanced数据，用于实验对比分析。</p>
<h3 id="网络模型设置与超参数配置"><a href="#网络模型设置与超参数配置" class="headerlink" title="网络模型设置与超参数配置"></a>网络模型设置与超参数配置</h3><p>训练过程中的超参数配置在table 1中给出。由于仿真数据集与MMT数据集的数量较多，因此batch size给的大一些。<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/%E8%B6%85%E5%8F%82%E6%95%B0.png"></p>
<p>在训练过程中，选择5取4的方式来进行训练，即5组训练数据中，选择4组进行training，1组进行validation，直到每一组数据都被作为一次validation。</p>
<p>对于EOS与MMT的imbalanced数据，类别权重被考虑仅loss function中，从来提升网络的性能。</p>
<p>迁移学习过程中，哪些层进行迁移，哪些层的参数会被fine-tuning，这些也是超参数的一部分，在table 2中进行了展示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/fine_tuning%E7%9A%84%E5%8F%82%E6%95%B0.png"></p>
<p>从表中可以看出，本文分别使用不同的数据集组合来进行预训练与fine-tuning，不同的迁移学习策略有着不同的迁移层。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="1D-CNN网络性能分析"><a href="#1D-CNN网络性能分析" class="headerlink" title="1D CNN网络性能分析"></a>1D CNN网络性能分析</h3><p>本文作者首先不进行迁移学习，直接在不同数据集上使用不同方法进行分类，结果如下表所示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/%E4%B8%8D%E5%90%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94.png"></p>
<p>从表中可以看出，支持向量机（SVM）在所有数据集上都有着最差的分类精度表现，FC方法（4层稠密全连接层）结果稍好一些，本文的1D-CNN网络有着最好的分类精度表现。1D CNN方法在仿真数据与EOS数据上的Confusion Matrix如下所示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/Confusion_Matrix.png"></p>
<p>在仿真数据上，13个类别的综合精度为84.4%，出现误分类的类别主要集中于形态较为近似的类别，如Cylinder类别与SpaceX。在EOS数据集上，9类目标的综合精度为75.3%，网络对于Spherical类别的分类效果最好，这是由于该类目标的光变曲线形态有着较强的独特性。与仿真数据的情况相同，误分类的问题主要集中在3类形状相似的火箭结构体类上（CZ-3B型、ARIANE 5型、FALCON 9型）。下表展示了在EOS数据集上的F1 Score，以及不同类别的数据所占的比例：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/EOS_F1.png"></p>
<p>从表格中可以看出，尽管对不平衡数据进行了加权，但分类效果仍然随着所占比例的减小而降低。</p>
<p>在未经过平衡处理的MMT数据集上，三种机器学习模型都取得了较高的总体分类精度。该数据集中75%的光变曲线来自于三个最大的类。本文作者的实验表明，1D CNN整体上由于前两种方法，实现了90.71%的交叉验证分类准确率，但在较小的类上表象相当差。从结果来看，类别数据量的不平衡性对于SVM、FC方法的影响更大，当使用平衡后的数据集时，二者的经度直接掉到了60%以下，而1D CNN方法则仍然保持有80.07%的综合分类精度。</p>
<p>在MMT平衡数据集上，尽管1D-CNN模型的总体分类准确率偏低，但模型在所有类上实现了类似的性能，如下图所示：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/Confusion_Matrix_MMT_B.png"></p>
<p>从图中可以看出，相比不平衡数据（文中只有EOS的Confusion Matrix），balanced数据能够使不同类的分类分类精度趋于一致。</p>
<p>下表对MMT平衡数据集的结果进行了更详细的分析，其中显示了F1分数、精度和召回率，以及该数据集中每个类别的unique目标的数量。<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/MMT_B_F1..png"></p>
<p>结果表明，该模型在LS-400类别上达到了最佳性能，LS-400是一个箱翼型卫星，其余为火箭体构型。</p>
<h3 id="迁移学习性能分析"><a href="#迁移学习性能分析" class="headerlink" title="迁移学习性能分析"></a>迁移学习性能分析</h3><p>下两图表示了迁移学习后的分类精度的变化：<br><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/Transfer_preformance_1.png"></p>
<p><img src="/../images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/Transfer_performance_2.png"></p>
<p>从图中可以看出，使用Blender预训练，然后将预训练模型迁移至真实数据集上的方法，相比直接使用真实数据集训练的方式，对模型的分类精度有着较高的提升。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>（本部分内容较多，直接翻译文章内容）<br>1D-CNN的多层结构能够学习出不同类型RSO的光变曲线的复杂决策边界，从而在真实数据上实现多目标分类。本文的实验结果支持了这一理论，在每个真实数据集上，1D-CNN的分类精度都明显高于FC神经网络和SVM。在Blender模拟数据集上的对象分类也出现了类似的结果。这表明，与之前研究中使用的FFM数据集相比，本研究中提出的Blender模拟数据集更复杂，仿真数据中类边界之间的划分更少。</p>
<p>根据Blender数据集的归一化混淆矩阵，本文1D CNN模型在区分相似对象方面存在一些困难。大约35%的圆柱体光曲线被错误地归类为SpaceX火箭体，还有相当大比例的SpaceX光曲线被错误地归类为圆柱体。这是意料之中的，因为这些模型都是相同的，除了SpaceX模型的鼻锥和喷嘴。对于所有的旋转轴和方向组合，这些特征对观察者来说都是不可见的，所以在这两个类之间会有许多非常相似的光曲线。同样，不同类型的立方体卫星以及平面的两种不同版本之间也存在一些错误分类。然而，在大多数情况下，模型不会将物体错误地分类为非常不同的形状。这增加了模型能够区分一般形状和特定对象的信心。</p>
<p>在MMT平衡数据集上的结果表明，本文模型一般能够区分不同类型的火箭构型。一个类别所包含的目标数量越少，模型性能越高。这表明本文模型能够学习出单个rso的特定特征，例如旋转周期，并使用这些信息来帮助对物体进行分类。</p>
<p>对于EOS、MMT两个真实数据集来说，类不平衡、训练样本数量的因素，与本文模型性能之间的明显相关性。这两个数据集都偏重于大型碎片，特别是火箭残骸，因为它们更容易被追踪，而且经常旋转。未来EOS的跟踪活动将侧重于从更广泛的对象中收集数据。由于收集真实数据的时间和成本，作者还将探索使用Blender模拟环境为具有有限真实光曲线数据的类模拟特定对象的想法。然后将这些模拟光曲线合并到训练数据中，以尝试改进真实光曲线的结果。由于大多数空间碎片物体可用的信息量有限，该工作的困难在于为Blender中的特定物体开发一个精确的形状及材质模型。</p>
<p>迁移学习是提高模型性能和整体分类精度的有效方法。在两个真实光曲线数据集上，当模型微调部分的训练数据集的大小受到限制时，迁移学习似乎是最有效的。随着训练数据量的增加，迁移学习的好处通常会减少。预计随着特定数据集的训练数据越来越多，这种趋势将持续下去，直到基线1D-CNN模型在没有迁移学习的情况下，与应用了迁移学习的1D-CNN持平或优于后者。然而，如前所述，获取和标记真实的光曲线数据是困难的，特别是对于广泛的类别。这项研究表明，迁移学习可以用来提高小型真实世界光曲线数据集的性能，并减少对长时间密集跟踪活动的需求。</p>
<p>在模拟Blender数据集上的预训练比在任何一个真实数据集上的预训练更有效。这大概是因为Blender的数据集比任何一个真实的数据集都要大得多，并且有更广泛的对象模型，而这两个真实的数据集严重偏向于火箭体类。这也可能是由于两个真实数据集之间的数据特征不同，MMT数据集通常比EOS数据集具有更高的采样率和更短的轨迹。</p>
<p>当目标数据集是MMT平衡数据集时，将有更多的层从预训练的网络中转移过来。传递四层表示卷积层和前两层全连接层都被传递了。这意味着，除了低层的一般特征外，在预训练期间学习到的高层的更具体的特征也被用于目标数据集上的最终网络。此外，在微调过程中，仅将最终输出层设置为可训练层，从而减少了微调时间和计算需求。相比之下，当EOS数据集作为迁移学习的目标数据集时，只有两个卷积层从预训练的网络中转移，并且在微调过程中将大多数层设置为可训练的。与MMT Balanced和Blender模拟数据集相比，这可能是EOS数据集中不平衡类的结果（最后这句话没看懂）。</p>
<p>与之前文献中模拟数据集的结果不同，即使加入迁移学习，在所有4个数据集上的分类结果也没有达到接近100%的准确率。就模拟数据而言，预期这一结果是由于用于生成模拟光曲线的输入参数的可变性增加而产生的。先前基于FFM的模拟对所有光曲线使用相同的初始历元、初始四元数和角速率，从而降低了分类任务的复杂性。在现实世界中，预计一定比例的光曲线将不能提供足够的信息来确定分类。这是由于方向轴与物体的复杂性之间的关系，以及1D光曲线数据中可观测信息的局限性。此外，为了使所提出的方法有效，实际光曲线必须足够精确，以使物体旋转引起的星等变化与噪声引起的变化区别开来。</p>
<p>综上所述，本文的提出的1D CNN及迁移学习的方法，被证明为是一个有效处理真实数据量不足与深度神经网络应用之间矛盾的方法。</p>
<p>[^1]: Allworth J, Windrim L, Bennett J, et al. A transfer learning approach to space debris classification using observational light curve data[J]. Acta Astronautica, 2021, 181: 301-315.</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2023/11/14/RealSense-SDK-rs2-context/"><img class="fill" src="/images/RealSense-SDK-rs2-context/Intel_RealSense_Logo_long.png" alt="RealSense SDK源码分析(1) rs2::context类"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-11-14T10:41:38.000Z" title="2023/11/14 18:41:38">2023-11-14</time>发表</span><span class="level-item"><time dateTime="2023-11-14T12:18:04.655Z" title="2023/11/14 20:18:04">2023-11-14</time>更新</span><span class="level-item">10 分钟读完 (大约1543个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/14/RealSense-SDK-rs2-context/">RealSense SDK源码分析(1) rs2::context类</a></p><div class="content"><p>High level API rs2::context中，核心的数据为rs2_context结构体。rs2::context中最为核心的query_devices()、query_all_sensors()等函数，均为调用了次级API(rs_xxx_xxx()一类的函数)。对这些函数来说，其输入是对应的次级结构体或类rs2_xxx。rs2_context即为这些次级结构的一员。</p>
<p>与rs2_context相关的次级API在rs_context.h和rs.cpp中实现。这些次级API，从本质上说，均调用了rs_context结构体中层级更低的librealsense::context类中的方法。librealsense::context类即是rs2::context对外API操作中，最低层的一级封装。</p>
<p>rs2_context包含了librealsense::context类的共享指针:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">rs2_context</span></span><br><span class="line">&#123;</span><br><span class="line">    ~<span class="built_in">rs2_context</span>() &#123; ctx-&gt;<span class="built_in">stop</span>(); &#125;</span><br><span class="line">    std::shared_ptr&lt;librealsense::context&gt; ctx;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>因此，rs2::context所有的具体操作，均调用了librealsense::context类中所对应的方法。</p>
<p>rs::context与librealsense::context之间的关系如下所示：</p>
<p><img src="/images/RealSense-SDK-rs2-context/class_relationship.png"></p>
<h2 id="librealsense-context类"><a href="#librealsense-context类" class="headerlink" title="librealsense::context类"></a>librealsense::context类</h2><p>该类如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">context</span> : <span class="keyword">public</span> std::enable_shared_from_this&lt;context&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">context</span><span class="params">( backend_type type )</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123; _device_watcher-&gt;<span class="built_in">stop</span>(); &#125;</span><br><span class="line">    ~<span class="built_in">context</span>();</span><br><span class="line">    std::vector&lt;std::shared_ptr&lt;device_info&gt;&gt; <span class="built_in">query_devices</span>(<span class="type">int</span> mask) <span class="type">const</span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> platform::backend&amp; <span class="title">get_backend</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> *_backend; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">uint64_t</span> <span class="title">register_internal_device_callback</span><span class="params">(devices_changed_callback_ptr callback)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">unregister_internal_device_callback</span><span class="params">(<span class="type">uint64_t</span> cb_id)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">set_devices_changed_callback</span><span class="params">(devices_changed_callback_ptr callback)</span></span>;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;std::shared_ptr&lt;device_info&gt;&gt; <span class="built_in">create_devices</span>(platform::backend_device_group devices,</span><br><span class="line">        <span class="type">const</span> std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt;&amp; playback_devices, <span class="type">int</span> mask) <span class="type">const</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::shared_ptr&lt;playback_device_info&gt; <span class="title">add_device</span><span class="params">(<span class="type">const</span> std::string&amp; file)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">remove_device</span><span class="params">(<span class="type">const</span> std::string&amp; file)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">add_software_device</span><span class="params">(std::shared_ptr&lt;device_info&gt; software_device)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">on_device_changed</span><span class="params">(platform::backend_device_group old,</span></span></span><br><span class="line"><span class="params"><span class="function">                            platform::backend_device_group curr,</span></span></span><br><span class="line"><span class="params"><span class="function">                            <span class="type">const</span> std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt;&amp; old_playback_devices,</span></span></span><br><span class="line"><span class="params"><span class="function">                            <span class="type">const</span> std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt;&amp; new_playback_devices)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">raise_devices_changed</span><span class="params">(<span class="type">const</span> std::vector&lt;rs2_device_info&gt;&amp; removed, <span class="type">const</span> std::vector&lt;rs2_device_info&gt;&amp; added)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">start_device_watcher</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    std::shared_ptr&lt;platform::backend&gt; _backend;</span><br><span class="line">    std::shared_ptr&lt;platform::device_watcher&gt; _device_watcher;</span><br><span class="line"></span><br><span class="line">    std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt; _playback_devices;</span><br><span class="line">    std::map&lt;<span class="type">uint64_t</span>, devices_changed_callback_ptr&gt; _devices_changed_callbacks;</span><br><span class="line"></span><br><span class="line">    devices_changed_callback_ptr _devices_changed_callback;</span><br><span class="line">    std::map&lt;<span class="type">int</span>, std::weak_ptr&lt;<span class="type">const</span> stream_interface&gt;&gt; _streams;</span><br><span class="line">    std::map&lt;<span class="type">int</span>, std::map&lt;<span class="type">int</span>, std::weak_ptr&lt;lazy&lt;rs2_extrinsics&gt;&gt;&gt;&gt; _extrinsics;</span><br><span class="line">    std::mutex _streams_mutex, _devices_changed_callbacks_mtx;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>该类的有8个类变量:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">std::shared_ptr&lt;platform::backend&gt; _backend;</span><br><span class="line">std::shared_ptr&lt;platform::device_watcher&gt; _device_watcher;</span><br><span class="line"></span><br><span class="line">std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt; _playback_devices;</span><br><span class="line">std::map&lt;<span class="type">uint64_t</span>, devices_changed_callback_ptr&gt; _devices_changed_callbacks;</span><br><span class="line"></span><br><span class="line">devices_changed_callback_ptr _devices_changed_callback;</span><br><span class="line">std::map&lt;<span class="type">int</span>, std::weak_ptr&lt;<span class="type">const</span> stream_interface&gt;&gt; _streams;</span><br><span class="line">std::map&lt;<span class="type">int</span>, std::map&lt;<span class="type">int</span>, std::weak_ptr&lt;lazy&lt;rs2_extrinsics&gt;&gt;&gt;&gt; _extrinsics;</span><br><span class="line">std::mutex _streams_mutex, _devices_changed_callbacks_mtx;</span><br></pre></td></tr></table></figure>
<p>该类的构造函数如下所示:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">context::<span class="built_in">context</span>( backend_type type )</span><br><span class="line">    : _devices_changed_callback(<span class="literal">nullptr</span>, [](rs2_devices_changed_callback*)&#123;&#125;)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">bool</span> version_logged=<span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (!version_logged)</span><br><span class="line">    &#123;</span><br><span class="line">        version_logged = <span class="literal">true</span>;</span><br><span class="line">        <span class="built_in">LOG_DEBUG</span>( <span class="string">&quot;Librealsense VERSION: &quot;</span> &lt;&lt; RS2_API_VERSION_STR );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    _backend = platform::<span class="built_in">create_backend</span>();</span><br><span class="line"></span><br><span class="line">    environment::<span class="built_in">get_instance</span>().<span class="built_in">set_time_service</span>(_backend-&gt;<span class="built_in">create_time_service</span>());</span><br><span class="line"></span><br><span class="line">    _device_watcher = _backend-&gt;<span class="built_in">create_device_watcher</span>();</span><br><span class="line">    <span class="built_in">assert</span>(_device_watcher-&gt;<span class="built_in">is_stopped</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输入参数为backend_type，但该参数貌似并没有用。构造函数首先初始化类成员变量_devices_changed_callback(TODO: 搞清楚这个类到底是什么)，接着调platform::create_backend()函数来为成员变量_backend赋值，然后调用类函数create_time_service()与create_device_watcher()，前者不知道在干什么(TODO: 弄清楚)，后者则为设备守望类型的成员变量_device_watcher赋值。最后，检查一下_device_watcher是否处于stop状态。</p>
<p>类成员变量_playback_devices、_devices_changed_callbacks、_streams、_extrinsics等在初始化是并未赋值，这些变量在调用到对应的函数时才会对其进行赋值。</p>
<h3 id="query-devices-与create-device-函数"><a href="#query-devices-与create-device-函数" class="headerlink" title="query_devices()与create_device()函数"></a>query_devices()与create_device()函数</h3><p>query_devices()是librealsense::context中非常重要的一个函数。该函数被调用时，返回当前连接的设备信息。该函数如下所示:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;std::shared_ptr&lt;device_info&gt;&gt; context::<span class="built_in">query_devices</span>(<span class="type">int</span> mask) <span class="type">const</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function">platform::backend_device_group <span class="title">devices</span><span class="params">(_backend-&gt;query_uvc_devices(), _backend-&gt;query_usb_devices(), _backend-&gt;query_hid_devices())</span></span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">create_devices</span>(devices, _playback_devices, mask);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>首先该函数创建了一个backend_device_group对象，将目前所有发现的设备集合起来。这些设备通过_backend对象的query_uvc&#x2F;usb&#x2F;hid_device()函数来获取。在得到设备的集合后，调用create_device()函数来创建对应的device_info对象。create_device()函数如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;std::shared_ptr&lt;device_info&gt;&gt; context::<span class="built_in">create_devices</span>(</span><br><span class="line">    platform::backend_device_group devices,</span><br><span class="line">    <span class="type">const</span> std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt;&amp; playback_devices,</span><br><span class="line">    <span class="type">int</span> mask) <span class="type">const</span></span><br><span class="line">&#123;</span><br><span class="line">    std::vector&lt;std::shared_ptr&lt;device_info&gt;&gt; list;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">LOG_INFO</span>(<span class="string">&quot;UVC backend: &quot;</span> &lt;&lt; devices.uvc_devices.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">LOG_INFO</span>(<span class="string">&quot;HID backend: &quot;</span> &lt;&lt; devices.hid_devices.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">LOG_INFO</span>(<span class="string">&quot;USB backend: &quot;</span> &lt;&lt; devices.usb_devices.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> t = <span class="built_in">const_cast</span>&lt;context*&gt;(<span class="keyword">this</span>); <span class="comment">// While generally a bad idea, we need to provide mutable reference to the devices</span></span><br><span class="line">    <span class="comment">// to allow them to modify context later on</span></span><br><span class="line">    <span class="keyword">auto</span> ctx = t-&gt;<span class="built_in">shared_from_this</span>();</span><br><span class="line">    <span class="comment">// 若mask为254, 那么下边的每一个循环都会执行一次, 但是pick_xxx_devices()中会对设备进行过滤, 非本系列产品线的设备返回一个长度为0的vector, 因此list中最终只保留对应的device_info</span></span><br><span class="line">    <span class="keyword">if</span> (mask &amp; RS2_PRODUCT_LINE_D400)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">LOG_INFO</span>(<span class="string">&quot;Create d400 device_info&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> d400_devices = d400_info::<span class="built_in">pick_d400_devices</span>(ctx, devices);</span><br><span class="line">        std::<span class="built_in">copy</span>(<span class="built_in">begin</span>(d400_devices), <span class="built_in">end</span>(d400_devices), std::<span class="built_in">back_inserter</span>(list));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( mask &amp; RS2_PRODUCT_LINE_L500 )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">LOG_INFO</span>(<span class="string">&quot;Create l500 device_info&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> l500_devices = l500_info::<span class="built_in">pick_l500_devices</span>(ctx, devices);</span><br><span class="line">        std::<span class="built_in">copy</span>(<span class="built_in">begin</span>(l500_devices), <span class="built_in">end</span>(l500_devices), std::<span class="built_in">back_inserter</span>(list));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mask &amp; RS2_PRODUCT_LINE_SR300)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">LOG_INFO</span>(<span class="string">&quot;Create sr300 device_info&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> sr300_devices = sr300_info::<span class="built_in">pick_sr300_devices</span>(ctx, devices.uvc_devices, devices.usb_devices);</span><br><span class="line">        std::<span class="built_in">copy</span>(<span class="built_in">begin</span>(sr300_devices), <span class="built_in">end</span>(sr300_devices), std::<span class="built_in">back_inserter</span>(list));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Supported recovery devices</span></span><br><span class="line">    <span class="keyword">if</span> (mask &amp; RS2_PRODUCT_LINE_D400 || mask &amp; RS2_PRODUCT_LINE_SR300 || mask &amp; RS2_PRODUCT_LINE_L500) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">LOG_INFO</span>(<span class="string">&quot;Create recovery devices&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> recovery_devices = fw_update_info::<span class="built_in">pick_recovery_devices</span>(ctx, devices.usb_devices, mask);</span><br><span class="line">        std::<span class="built_in">copy</span>(<span class="built_in">begin</span>(recovery_devices), <span class="built_in">end</span>(recovery_devices), std::<span class="built_in">back_inserter</span>(list));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mask &amp; RS2_PRODUCT_LINE_NON_INTEL)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">LOG_INFO</span>(<span class="string">&quot;Create uvc_device_info&quot;</span>);</span><br><span class="line">        <span class="keyword">auto</span> uvc_devices = platform_camera_info::<span class="built_in">pick_uvc_devices</span>(ctx, devices.uvc_devices);</span><br><span class="line">        std::<span class="built_in">copy</span>(<span class="built_in">begin</span>(uvc_devices), <span class="built_in">end</span>(uvc_devices), std::<span class="built_in">back_inserter</span>(list));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp;&amp; item : playback_devices)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">auto</span> dev = item.second.<span class="built_in">lock</span>())</span><br><span class="line">            list.<span class="built_in">push_back</span>(dev);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (list.<span class="built_in">size</span>())</span><br><span class="line">        <span class="built_in">LOG_INFO</span>( <span class="string">&quot;Found &quot;</span> &lt;&lt; list.<span class="built_in">size</span>() &lt;&lt; <span class="string">&quot; RealSense devices (mask 0x&quot;</span> &lt;&lt; std::hex &lt;&lt; mask &lt;&lt; <span class="string">&quot;)&quot;</span> );</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> list;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>create_device()</code>函数的输入参数有三个：</p>
<ol>
<li><code>platform::backend_device_group</code>对象，创建<code>device_info</code>对象时使用;</li>
<li><code>std::map&lt;std::string, std::weak_ptr&lt;device_info&gt;&gt;</code>类型的类内变量，即当前context对象中通过手动方式添加的设备;</li>
<li><code>int</code>类型变量<code>mask</code>，用来判断输出哪种类型设备的<code>device_info</code>列表。</li>
</ol>
<p><code>create_device()</code>函数首先根据<code>mask</code>值，判定创建哪一类型的<code>device_info</code>。在<code>rs2::context</code>中，通常不会指定<code>mask</code>的值，当未指定时，该值会被赋为254，所有的<code>if</code>都会走一变。为了防止创建出并未连接的设备，<code>device_info</code>各种派生类的<code>pick_xxxx_devices()</code>函数中，都会首先通过设备pid来进行筛选，符合系列pid的设备才会被创建。</p>
<p>连接一个D455设备时，将会返回3个device_info组成的队列。</p>
<h2 id="rs2-context-query-devices-的过程"><a href="#rs2-context-query-devices-的过程" class="headerlink" title="rs2::context::query_devices()的过程"></a>rs2::context::query_devices()的过程</h2><p>综上，<code>rs2::context</code>对象轮询所有设备的过程如下所示:<br><img src="/images/RealSense-SDK-rs2-context/query_devices.png"></p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2023/10/04/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/"><img class="fill" src="/images/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/cover.png" alt="Deep learning-based spacecraft relative navigation methods A survey"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-10-04T07:30:36.000Z" title="2023/10/4 15:30:36">2023-10-04</time>发表</span><span class="level-item"><time dateTime="2023-10-04T12:48:58.344Z" title="2023/10/4 20:48:58">2023-10-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Navigation/">Navigation</a></span><span class="level-item">10 分钟读完 (大约1540个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/04/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/">Deep learning-based spacecraft relative navigation methods A survey</a></p><div class="content"><p>本文是基于深度学习的飞行器相对导航算法的综述。相比地面任务，飞行器导航需要高可靠性，同时又缺乏大规模的数据集。本文围绕飞行器交互对接（spacecraft rendezvous）、小天体探索、地形导航三个方面，对现有的DL-Based算法进行介绍。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>相机传感器地位的提升，使得Deep-Learning方法在宇航任务中存在应用的可能。在未来，DL-Based方法将在以下三个领域实现广泛应用：</p>
<ol>
<li>非合作目标的交互对接(Non-Cooperative rendezvous)；</li>
<li>降落与着陆过程的地形导航；</li>
<li>小天体的探索与patch pinpoint点定位</li>
</ol>
<p>在前人的研究中，Kothari等人的论文中主要讨论了spacecraft positioning onboard system在入坞(docking)与着落(landing)过程中所应该实现的目标。Cassinis(注意不是Cassini号)等人则针对非合作飞行器交会对接方面，首次对DL方法进行了整理与综述。</p>
<p>下图中，从上述三个应用方向，对现有的DL-Based相对定位方法进行了整理。</p>
<p><img src="/images/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/DL-Based_SC_relative_navigation.png"></p>
<blockquote>
<p>注意，图中的代表方法都以参考文献的方式写出，需要到对应的参考文献列表中查阅论文</p>
</blockquote>
<h2 id="相对导航中的DL位姿估计算法"><a href="#相对导航中的DL位姿估计算法" class="headerlink" title="相对导航中的DL位姿估计算法"></a>相对导航中的DL位姿估计算法</h2><p>本部分主要介绍S&#x2F;C rendezvous中基于深度学习的相对导航算法。位姿估计算法分为直接法与间接法，直接法即E2E的网络，网络输入图像，输出位姿，间接法则将位姿估计分为特征提取与位姿估计两部分，输入图像先提取特征，再通过另一中方法或另一个网络来估计出位置与姿态，如下图所示：</p>
<p><img src="/images/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/direct_and_indirect_method.png"></p>
<h3 id="地基DL相对定位算法相关研究"><a href="#地基DL相对定位算法相关研究" class="headerlink" title="地基DL相对定位算法相关研究"></a>地基DL相对定位算法相关研究</h3><p>讨论深度学习算法，离不开地基相对位姿估计算法的研究，因为大部分情况下，深度学习的飞行器位姿估计算法都是从现有的地基载具定位算法发展而来。本文中提到的方法如下：</p>
<ol>
<li>Kendall[], PoseNet，将GoogleNet中的softmax分类器换为affine regressor，实现对姿态四元数和位置向量的估计，该方法在大尺度outdoor场景下能够实现2m、3deg的精度，在indoor场景下能够实现0.5m、5deg的精度；</li>
<li>Wang[]，DeepVO，一个单目视觉里程计(Monocular Visual Odometry)，这个方法的pipeline为一个DRCNN结构[]，首先使用一个预训练的FlowNet从RGB图像序列中提取出特征，接着使用LSTM对特征进行处理，估计出位姿，本文端到端的DRCNN能够将整个轨迹的位置和姿态RMSE控制在5.96%和6.12deg（论文中所提到的条件的100m-800m长度的轨迹，这里尚未弄清楚该轨迹是什么轨迹）；</li>
<li>Rad and Lepetit[]，BB8，使用卷积网络来回归8个三维空间点的二维位置，用来定义PnP算法的bounding box，使用PnP算法来恢复位姿，文章中使用VGG来作为基础网络，使用经典的重投影误差来作为loss function。</li>
</ol>
<h3 id="空间DL相对定位算法面临的挑战"><a href="#空间DL相对定位算法面临的挑战" class="headerlink" title="空间DL相对定位算法面临的挑战"></a>空间DL相对定位算法面临的挑战</h3><p>地基适用的算法与空间适用的算法依然有着很大的区别。这些区别主要是由于空间环境、飞行器本身条件所导致的，主要有以下六个：</p>
<ol>
<li>行星与恒星运动对导航系统产生的干扰（这里应该说的是相对的，因为恒星的绝对位置认为是不动的）</li>
<li>缺少大气和光线的散射，对成像条件形成了挑战</li>
<li>更strong的阴影与光比更大的光照条件，会产生对比度更为极端、信噪比更低的图像</li>
<li>星载硬件的功率计算资源的匮乏使得DL算法很难处理</li>
<li>训练数据集几乎没有（scarce）</li>
<li>对DL技术可靠性的担忧</li>
</ol>
<p>事实上，相比传统方法，深度学习方法一定程度上能够减轻上述的挑战（文章作者观点，本人持有“Talk is cheap, show me code”的态度）。例如，基于DNN的算法在动态光照条件下表现出更高的鲁棒性，部署高性能处理设备（可能是指Nvidia Jetson系列的开发板）不仅能够减小计算复杂度，同时还可以降低复杂动态模型的必要性（Need of complicated dynamic models），对于定位结果的可靠性，DL算法也可以输出不同的结果，通过融合其他传感器的数据——例如使用滤波算法——提高定位的可靠性。</p>
<p>ESA在2019年发起了KPEC(Kelvins Pose Estimate Challenge)竞赛，目的在于吸引研究者们开发适用于人造卫星相对定位的算法。该竞赛使用两个数据集，其中一个为斯坦福大学构建的SPEED(Spacecraft Pose Estimation Dataset)仿真合成数据集，另一个是在实验室中进行缩比模拟的数据集。</p>
<blockquote>
<p>查一查这个数据集是怎么做出来的<br>ESA的KPEC竞赛在2021年也办了一次，这次他们对数据集进行了扩展</p>
</blockquote>
<h3 id="直接法空间飞行器相对姿态估计算法"><a href="#直接法空间飞行器相对姿态估计算法" class="headerlink" title="直接法空间飞行器相对姿态估计算法"></a>直接法空间飞行器相对姿态估计算法</h3><p>Spacecraft rendezvous如下图所示：</p>
<p><img src="/images/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/chaser.png"></p>
<p>相对位姿估计的最终目的，是从图像序列中获取目标target在chaser坐标系$\mathop{\mathcal{F}}\limits_{\rightarrow^{c}}$下的6自由度姿态$\mathop{\mathcal{F}}\limits_{\rightarrow^{t}}$。</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2023/09/22/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/"><img class="fill" src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/BennuAsteroid.jpg" alt="Digital terrain mapping by the OSIRIS-REx mission"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-09-22T12:56:46.000Z" title="2023/9/22 20:56:46">2023-09-22</time>发表</span><span class="level-item"><time dateTime="2023-09-22T14:03:05.967Z" title="2023/9/22 22:03:05">2023-09-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Asteroid-Mission/">Asteroid Mission</a></span><span class="level-item">1 小时读完 (大约6815个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/09/22/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/">Digital terrain mapping by the OSIRIS-REx mission</a></p><div class="content"><p>本文对OSIRIS-REx任务中，DTM及相关数据铲平生产所安排的观测计划，以及DTM数据生产所使用的技术进行了综述。本文所介绍的内容，发生在<strong>The Use of Digital Terrain Models for Natural Feature Tracking at Asteroid Bennu</strong>之前。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>OSIRIS-REx对Bennu探测的任务中，生成global&#x2F;local digital terrain models(DTMs)是十分重要的一环。DTM有着重要的科学意义和工程意义，是任务中十分重要的数据产出。</p>
<p>DTM数据生产的任务由Altimetry Working Group(AltWG)负责。除DTM数据之外，该任务组还会生产出surface tile map和geodetic products(例如倾斜、重力图等)。大地测量数据对于生产 和 有着至关重要的作用。除此之外，AltWG还会生产1064nm反射地图（使用OLA激光测距仪得到）和相对反照率地图（使用OCAMS相机得到）。</p>
<p>DTM数据的意义包括科学意义和工程意义。</p>
<h3 id="科学意义"><a href="#科学意义" class="headerlink" title="科学意义"></a>科学意义</h3><p>所谓科学意义，是指DTM对于科学研究方面的一些好处。</p>
<h4 id="揭示Bennu的演化"><a href="#揭示Bennu的演化" class="headerlink" title="揭示Bennu的演化"></a>揭示Bennu的演化</h4><p>DTM数据对于推断小天体的地质起源和演化是至关重要的，例如，通过无线电测量得到的精确的体积和密度，来对小天体体积密度进行限制（这大概是说，通过无线电测量结果，就能对小天体的体积密度进行一个估计），而小天体的体积密度则是Bennu是否为有孔天体(nature of the porosity within Bennu)的关键线索，这将反过来揭示小天体的起源、碰撞和演化。</p>
<h4 id="提供容貌信息"><a href="#提供容貌信息" class="headerlink" title="提供容貌信息"></a>提供容貌信息</h4><p>Global DTMs在天体容貌特性研究中同样具有至关重要对作用(necessary)。它能够反映出地表轮廓线是否具有可测量地形，并为“确定这些轮廓线与其余特征之间（例如陨坑、鹅卵石）的关系”提供了背景信息（context）。轮廓线信息能够帮助研究天体的内在自然特性。</p>
<h4 id="研究采样点风化情况"><a href="#研究采样点风化情况" class="headerlink" title="研究采样点风化情况"></a>研究采样点风化情况</h4><p>OSIRIS-REx的重要任务是Touch-and-Go定点采样。DTM能沟通提供地质学中关于表面风化影响的一些信息。采样点的重力势面（geopotential slope）、几何势面分布（geometric tile distribution）以及石块的几何高程（geometric height）对于理解采样点的地质学特性具有至关重要的作用。同时，采样点位置的上述数据产品，能够通过最大化OSIRIS-REx采样系统的表现，来尽可能减少风险。</p>
<h3 id="工程意义"><a href="#工程意义" class="headerlink" title="工程意义"></a>工程意义</h3><p>DTM对于OSIRIS-REx的工程意义，在于光学导航方面。</p>
<p>DTM数据对于飞行器的导航有着重要作用。OSIRIS-REx任务在接近过程中，始终采用光学导航的方式。研究者们开发了一套称为”NFT’的<strong>Natural Feature Tracking|自动导航方法（Natural Feature Tracking，NFT</strong>。该方法中，需要使“自然特征”，在最后的采样阶段使用该方法。</p>
<p>OSIRIS-REx的导航过程中，均需要使用高精度的landmark navigation(MLNs)地图，这些地图包含了DTM的表面高程数据和相关的反照率信息。在任务的早期阶段，飞行器会收集生成MLN所需的数据，而MLS则在后续的任务阶段中，被Flight Dynamics System team所使用。早期阶段的数据包括图像和激光测距仪测量得到的数据，这些数据经过Approach imaging campaign和几次双曲线飞掠过程获得。关于具体的测绘规划和轨道信息，未来将在<strong>Bennu飞掠测量任务轨道</strong>中给出详细的整理。</p>
<h2 id="Observation"><a href="#Observation" class="headerlink" title="Observation"></a>Observation</h2><p>论文第二部分Observation中，介绍了DTM数据生成所进行的两种观测计划。为了得到生成DTM所需的数据，OSIRIS-REx将观测计划分成了两个部分：</p>
<ul>
<li>基于OCAMS相机套件的被动观测，利用Stereophotoclinometry, SPC方法来建立模型</li>
<li>基于OLA的激光套件的主动观测，直接利用激光点云建立模型</li>
</ul>
<p>OCAMS和OLA的观测数据被各自独立使用，分别生成DTM和相关数据，这种方式的好处是，能够对最终生成的数据提供一个相互检验的途径。</p>
<p>OCAMS和OLA观测数据，能够产生分辨率约为0.75m，绝对精度小于0.5m的DTM数据。同时，它们也可以提供生成更高精度DTM的数据，例如全小天体的0.32m或者0.08m分辨率DTM。两类传感器中，尽管OLA数据是sample-site附近0.05m以下分辨率DTM数据的主要来源，但是两种传感器的数据仍然都可以使用，来生成DTM。进一步，OCAMS和OLA分别得到的数据集，能够各自独立第完成对小天体的极区位置、旋转周期，以及形心-重心偏移量。OLA和OCAMS数据同样能够为NFT提供至多300个MLN的特征点。在NFT所需的特征中，需要包含多个0.08m分辨率的位置。</p>
<p>综上，OCAMS和OLA的观测计划，能够提供如下的数据：</p>
<ul>
<li>生成0.75m分辨率、绝对精度&lt;0.5m的全局DTM所需的数据（各自独立地）；</li>
<li>生成0.32m或0.08m分辨率的整星DTM所需的数据（文中没有说是否是各自独立地生成数据）；</li>
<li>300个MLNs，NFT导航用。</li>
</ul>
<p>接下来就分别对OCAMS和OLA的观测计划进行详细对介绍。</p>
<h3 id="OCAMS-Imaging"><a href="#OCAMS-Imaging" class="headerlink" title="OCAMS Imaging"></a>OCAMS Imaging</h3><p>OCAMS观测是OSIRIS-REx非常重要的任务规划，OCAMS将采集图像数据，以便使用SPC方法来进行形状建模。这部分观测将在接近段Approach、预观测段Preliminary Survey、Oribit A，以及细节观测段Detailed Survey来进行。观测计划如下图所示：</p>
<p><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/OCAMS%E8%A7%82%E6%B5%8B%E8%AE%A1%E5%88%92.png"></p>
<p>总的来说，OCAMS+SPC建模的过程为：<br><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/OCAMS%E8%A7%82%E6%B5%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png"><br>其中：</p>
<blockquote>
<ol>
<li>关于各个观测计划的轨道信息，需要阅读OSIRIS-REx的其他文献</li>
<li>SPC建模中，共需要五幅图，分别为东西南北四个方向的图像和一副low phase的图像</li>
</ol>
</blockquote>
<h3 id="OLA-Observations"><a href="#OLA-Observations" class="headerlink" title="OLA Observations"></a>OLA Observations</h3><p>使用激光测距仪OLA进行观测主要有两个目的：</p>
<ul>
<li>在任务早期阶段，飞行器距离Bennu相对较远时(~7km)，为SPC Modelling提供小天体尺度确认，以及独立的飞行器导航结果；</li>
<li>在之后的阶段，由于可以生成极高精度的DTMs(GSD 0.08m global, &lt;0.05m locally for sample-sites)，OLA的扫描能力可以显著地降低任务时间。</li>
</ul>
<p>早期的OLA测量结果对于确定SPC和导航过程中的相机问题或者bias骑着至关重要的作用[^1]。</p>
<p>[^1]: 经验显示，SPC建模过程中，会由于焦距的不确定性和几遍而导致小概率的问题。此外，飞行器相对目标的指向及范围的不确定性会造成SPC的degeneracy solution。</p>
<blockquote>
<p>特别注意，当SPC产生degreneracy解时，基于SPC估计得到飞行器的指向和范围是不容易被deconvolved(反卷积)，这句话不太明白是什么意思。</p>
</blockquote>
<p>SPC产生degeneracy解时，会导致对小天体模型整体尺寸估计的不确定。纠正这种不确定性可以采用多种技术手段，例如使用radio science tracking data来得到飞行器的速度信息，在apporach阶段，将飞行器速度与图像进行匹配，从而消除尺度估计的不确定性。相比上述方法，直接使用OLA测距仪是非常省时节力的方法，特别是在任务的前期阶段，任何对数据产品的调整都是非常迅速的，对下游的产品的影响是非常小的。这种激光测距信息再JAXA的隼鸟号“丝川”小天体探测任务中起到了非常重要的作用。</p>
<p>飞行器的导航过程也从激光测距信息中获益匪浅。激光测距仪为飞行器提供了额外的绝对尺度信息，对识别由于太阳热辐射撞击（可能是太阳风）、漏气导致的小作用力产生的影响有这很好的帮助。这些微小作用力很难对它们进行精确建模，但会持续影响飞行轨迹，使得飞行器偏离目标。</p>
<p>OLA在1km高度的操作是相对简单的。在这个距离下，单个OLA光栅扫描器可以提供Bennu表面的精确DTM。若干个这种数据融合，或者一次225m高度飞掠扫描成像，就可以提供预期采样点的GSD 0.05m DTM数据。一组在1km轨道所采集的OLA扫描数据，能够满足可采样性、安全性、NFT interest的需求，即构建一个GSD 0.05m的DTM数据。如下图所示。</p>
<p><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/OLA%E6%89%AB%E6%8F%8F%E6%95%B0%E6%8D%AE%E5%9B%BE.jpg"></p>
<p>仿真实验结果显示，在高度为750m的轨道上，经过为期一个月的数据收集，能够制作0.08m分辨率的Gloabl DTM。在这个DTM上，大约每0.08m的bin至少需要包含了5次返回（retuens）。多次返回能够减少垂直方向上DTM的不确定性。这意味着，即便考略到一些相当大的飞行器漂移，该分辨率的DTM数据仍然能够在80%左右的小天体表面获得。类似的，之前所述的拟采样点225m高度飞掠过程，获取的GSD 0.03m DTM中，每0.025-m bin的返回次数大致相同（是不是说与上述的0.08-m bin 5次返回一样）。</p>
<blockquote>
<p>在原文第三段的最后，又提到了使用SPC去构建相同的、能够满足飞行器的安全性和采样需求的全局和局部DTMs，需要更为复杂的贴近观测计划，这个复杂性是由SPC所需的分辨率和观测方位所决定的。作者的结论似乎是实施纯SPC建模任务，会导致图像采集计划变得复杂，会对任务的人力资源、成本和日程安排造成巨大压力，（因此需要OLA来去做辅助，帮助缩短建图时间，个人理解）。</p>
</blockquote>
<p>OLA观测计划如下图所示：</p>
<p><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/OLA%E8%A7%82%E6%B5%8B%E8%AE%A1%E5%88%92.jpg"></p>
<p>OLA观测的具体细节如下图所整理：<br><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/OLA%E5%9C%B0%E5%BD%A2%E6%89%AB%E6%8F%8F%E5%BB%BA%E6%A8%A1.png"></p>
<h2 id="DTM-development"><a href="#DTM-development" class="headerlink" title="DTM development"></a>DTM development</h2><p>由上所述，DTM的生成采用两种方法：基于图像数据的光度立体法（SPC）和基于激光雷达数据的建模方法。</p>
<h3 id="Stereophotoclinometry"><a href="#Stereophotoclinometry" class="headerlink" title="Stereophotoclinometry"></a>Stereophotoclinometry</h3><p>SPC在1988年被用来对表面进行建模。该方法使用不同光照方向和观测方向的图像数据，来生成卫星、小天体、行星、彗星的地形和形状模型。火卫一、Phoebe、火星、水星、月球、灶神星、Itokawa、67P彗星等都用SPC建过模。如下图所示：</p>
<p><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/SPC%E5%BB%BA%E6%A8%A1%E5%A4%A9%E4%BD%93.png"></p>
<p>SPC最初是作为光学导航工具的一部分而被研发出来，用来使飞行器在几乎任何光照和观测条件下都能够识别出天体表面特定的路标或控制点。有了这些路标位置，通过三角花，可以对飞行器相对观测体的轨迹和指向进行估计和计算。在这种导航过程中，飞行器观测下的天体表面形状和相对反照率需要被很好地建模。通过结合光度倾斜测量和立体几何方法（Combining geometric stereo techniques with photoclinometry），同时利用着色和光照方向，SPC能够生成上述所需的模型。SPC方法已经成功的运用在了Itokawa、Vesta、Ceres探索任务的导航上。</p>
<p>SPC能够使用（几乎）所有的图像，无关分辨率。SPC能够使用远距离低分辨率的图像（典型值为目标占据150-300像素）来构建初始模型，用来获取两极位置（其实也就是自转轴）、自转速度等信息的估计。在OSIRIS-REx任务中，使用边缘观测（limb observations）来构建第一个模型。作者团队将这些由limb部分构件的模型，与SPC处理过程中的图像相结合，从而生成初始的低分辨率的DTMs（又被称为maplets）或路标。利用这些路标，可以判断出目标自转轴倾角和自转速率。在获取更多的图像数据之后，就会构建出额外的DTMs，生成更好的形状模型，同时更新自转轴倾角和自转速率。这些改进反过来提供了一个初始模型，用于生成具有越来越高的GSD的DTM，这是因为在接近Bennu的过程中，可以获得越来越高分辨率的图像。</p>
<p>在使用limb observation构建出边缘模型之后，SPC使用立体视差来建立表面斜坡、表面亮度、表面相对反照率之间的关系。使用一系列目标大小超过300像素的图像对小天体进行建模，能够达到上述的标准。同时，多幅图像能够提供观测的不同方位角和高度角，以实现天体表面的离散可识别特征点的立体视觉。这些立体几何数据定义了分布在目标小天体或者行星上的许多maplets的中心像素。目标的旋转提供了表面上的任意点的不同光照信息，这使得如下两种行为成为可能：1）利用maplet建立倾斜度和亮度之间的关系；2）提供表面的相对反照率信息。倾斜度和亮度之间的关系，采用相应的光度学函数来进行描述，特别地，一种修正的Lommel-Seeliger函数被广泛应用于小天体研究中。测试结果表明，不同的光度函数对最终的数据产物有着微乎其微的影响（have little impact on the final products），这主要是由于SPC最佳的相位范围为50~120°，这个相位范围，大多数光度函数是相似的。SPC使用光度函数，利用所有的图像数据，通过求解优化问题的线性估计解来估算maplet中每个像素的表面倾斜度，该优化问题将最小化maplet中每个像素的亮度平方和的残差，计算过程最少使用5张图像，最多使用几百张图像。得到倾斜度之后，就可以生成高度，从而得到每个maplet的表面形状数据，通常为100 pixel $\times$ 100 pixel的局部DTM。</p>
<p>为了得到Global Model，需要将上述得到的局部DTMs贴合在一起。完成上述工作主要依靠maplets之间的相互重合，以及之前所提到的边缘观测（limb observations），以及全局立体视差对每个maplets定义的中心位置。这些数据将提供maplets之间的tilt-to-height积分的约束，用来对global表面进行估计。用于闭合全局模型的最小二乘反演为飞行器的状态和目标的形状提供了一些形式上的不确定性。最后，SPC将会提供目标中心在图像的位置、两极位置、晃动、自转状态、体积等信息。</p>
<p>前文中已经提到，根据测试过程时的经验，为了得到最好的结果，SPC对一块地行进行建模，至少需要4幅不同观测高度角&#x2F;方位角的图片，在相反的E-W和N-S象限约呈90°角分开<a href="%E8%BF%99%E9%87%8C%E7%A1%AE%E8%AE%A4%E4%B8%80%E4%B8%8B%E5%88%B0%E5%BA%95%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%86%E5%B8%83%EF%BC%8C%E8%AE%BA%E6%96%87%E9%87%8C%E8%AF%B4%E7%9A%84%E6%9C%89%E7%82%B9%E6%8A%BD%E8%B1%A1">^2</a>。图片的太阳角度也应像观测角一样具有分布差异。这些太阳角需要位于东边或西边45°。一些南北方向的太阳方向对于带有一定自转倾角的小天体或者行星是非常有帮助的，它能够消除近极区的阴影的影响。除此以外，第五幅约0°出射角、10°入射角的图像能够获取所必需的相对反照率信息。所有的图像都需要与空间分辨率进行比较，至少需要1~2幅图拥有高于期望GSD的分辨率，而其他图像则不应低过该GSD约5倍因子乘积。<a href="%E5%8E%9F%E6%96%87%E4%B8%AD%E7%9A%84%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%AFlow%EF%BC%8C%E4%BD%8E%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%AFhigh%EF%BC%8C%E8%AF%B4%E7%9A%84%E6%98%AF%E7%BB%9D%E5%AF%B9%E7%9A%84%E6%95%B0%E5%80%BC">^3</a></p>
<p>SPC建模的精度最终取决于观测条件。本文团队使用预测轨迹和观测设计，并使用重建的飞行器姿态和轨迹来苹果获得观测后的结果。更进一步，使用internal SPC metrics来对模型精度进行评估。Internal SPC metrics能够确定实拍图像和利用生成模型渲染得到的图像之间的残差或者差异。当使用组合生成全局形状模型时，还会从SPC中使用的最小二乘反演和maplets之间的内部一致性推导出形式不确定性<a href="%E8%BF%99%E9%87%8C%E8%AF%B4%E7%9A%84%E5%BD%A2%E5%BC%8F%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%98%AF%E6%8C%87%E4%BB%80%E4%B9%88">^4</a>。此外，一个独立的正规化交叉相关技术（an independent normalized cross-correlation technique）被用来评估SPC得到的表面高度、反照率结果的好坏。在相关性测试中，并不使用SPC过程中所使用到的图像。这种保护措施维持了结果评估和模型生产之间的独立性。上述的交叉相关测试能够获取模型的水平精度，以及较小程度上的垂直精度<a href="%E8%BF%99%E9%87%8C%E7%9A%84%E6%B0%B4%E5%B9%B3%E7%B2%BE%E5%BA%A6%E5%92%8C%E5%9E%82%E7%9B%B4%E7%B2%BE%E5%BA%A6%E5%8F%88%E6%98%AF%E4%BB%80%E4%B9%88">^5</a>。OLA数据提供了另一个评估SPC产品的方式，OLA能够直接测量出到表面的距离，通过比较与SPC DTMs渲染出的图像及反照率之间的尺度差异，该距离信息将作为一个约束，用来评估SPC DTMs的优劣。最后，一系列的“truth”模型测试，包括视角集合评估（viewing geometry assessments）、SPC internal metrics、交叉相关结果，提供了一个在轨预测绝对精度和DTMs精确性的途径。</p>
<p><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/SPCModel%E8%BF%87%E7%A8%8B%E4%B8%8E%E8%AF%84%E4%BC%B0.png"></p>
<h3 id="Tiling-OLA-scans"><a href="#Tiling-OLA-scans" class="headerlink" title="Tiling OLA scans"></a>Tiling OLA scans</h3><p>OLA扫描数据的tiling（铺设，平铺）包含两个步骤：</p>
<ul>
<li>使用SIFT特征描述子来寻找出独立OLA扫描数据之间的重叠特征&#x2F;关键点</li>
<li>使用ICP方法来匹配两块OLA扫描数据</li>
</ul>
<p>对于特征点过程，工作在OLA数据上。首先对OLA数据进行滤波，获得噪声返回值（the approach begins by first filtering the OLA data for any noisy returns）(总之就是先滤波)。接着局部DTMs以每个OLA扫描为中心被构建出来。点云在3D空间中的初始位置通过飞行器的位置和指向、OLA扫描探头指向、OLA测量距离确定。<strong>为了方便分析，假设局部DTM为刚体</strong>，因为OLA mirror是具有高度稳定性的。每个OLA DTM的拉普拉斯算子提供了一个2D图像，显示了曲面曲率随距离的变化率。大量的证据表明，当视角中有不同对斜坡时，<font color = green>Laplacian算子</font>[^6]在识别匹配表面特征方面也有一定的用处。</p>
<p>[^6]: 如果这里的拉普拉斯算子是通常意义的算子的话，梯度的散度，或者$f$在当前位置$x &#x3D; (x_{1}, … , x_{n})$的二阶偏导之和$\sum_{i &#x3D; 1}^{n}\limits{\frac{\partial^{2} f}{\partial x_i^2}}$ </p>
<p>Laplacian算子得到的图像很适合用SIFT算法来进行匹配。SIFT能够在较大的几何范围和光度范围内对特征进行匹配。SIFT特征匹配完成后，通过滤波去除一些outliers，就可以用来计算3D刚体旋转和平移，从而将OLA扫描数据的重叠部分进行配准。配准的过程还需在整星范围进行迭代，从而使得重叠的OLA扫描的残差最小。最后需要进行最终的调整，同时对所有的OLA扫描进行变换，最小化OLA扫描和关键点调整得出的飞行器轨迹估计-FDS得到的飞行器轨迹估计之间的差异。其中，FDS使用SPC生成的landmark来进行轨迹估计，但它还会将radio science results得到的重力影响考虑到算法中。最后一步是确定OLA生成的形状模型，使得Bennu的质心与FDS得到的结果一致。上述基于SIFT的过程如下图所示：</p>
<p><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/%E5%9F%BA%E4%BA%8ESIFT%E7%9A%84OLA%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B.jpg"></p>
<p>ICP是基于OLA数据生成形状模型的第二种方法，主要用在当keypoint方法寻找匹配失败时，这些失败往往是由于OLA扫描没有足够的overlap，或者是不平坦的数据（unevenly spaced data）。此外， ICP也可以用来将点云匹配到SPC生成的模型上，用来在轨评估SPC生成模型的质量，并对该模型质量进行进一步的提升。相比快速的特征点匹配，ICP是一个计算密集型算法，因为它要迭代地最小化两块点云的差异，来计算出位姿变换，这个计算过程中需要使用到非常多的点云中的点。OSIRIS-REx任务使用的ICP算法，使用最小化均方根误差（RMS）来对点云进行匹配。ICP可能会使用所有的3D点，因此除移除噪声点之外，不会有任何的预处理过程。一旦将outliers进行移除，就可以确定出OLA扫描之间的重叠部分，从而使用subregion(应该指的是这部分重叠区域)来对两块点云进行匹配，计算出平移和旋转。在ICP算法中，识别重叠区域是非常重要的，这样可以有效避免得到不正确的局部最优解。</p>
<p>在所有OLA数据的都矫正完成之后，就可以用来构建全局&#x2F;局部地形模型。根据Bennu的球度（degree of sphericity），将采用1～2个步骤来构建DTM。Bennu接近于球体，将OLA数据装配在全局经纬度bin上，这些网格的大小为模型期望GSD尺寸的一半。当构建出300万个面的形状模型时，每个bin大概0.30m。算法将通过直接从0.30m网格像素内获取的所有adjusted OLA data中计算median radius，来生成全局表面地图。在那些没有OLA数据的bin中，会使用tension-based spline fit插值来补充完整。装配完成的网格地图将会被重新采样，以生成3D形状模型，这样，就可以在天体表面上生成几乎相等面积的facets。为了应付更通用的情况，例如细长型天体（Eros）、极度凹型天体（67P）、具有悬臂结构的情况，又构建了一系列的小的局部地图，称为mapolas。（在各自完成构建之后），这些mapolas将使用SPC或AltWG开发的替代算法，再次融合成更广的数据产品（例如小天体形状模型、全局&#x2F;降落点的地形图），其中在局部法线的方向进行调整，平滑全局形状模型，以匹配局部mapolas。</p>
<p>对ROI(Region of Interest)的条带调整[^7]OLA数据进行采样，能够直接生成更高分辨率局部数据产品。对整个小天体的局部OLA采样最终将生成0.3~0.08m分辨率的DTMs。对采样点，生成的是GSD 0.05m的map。依靠OLA传感器，极高分辨率的map（约GSD 0.03m）同样是可行的。</p>
<p>基于OLA生成的形状模型的精确性和准确性依赖条带调整的准确度。在飞行过程中，条带调整后独立OLA点之间的标准误差矩阵是非常关键的一个指标。OSIRIS-REx任务中，将会在一块maploa中给定的bin上测量该误差。最终，authors further determine the quality of adjustment to scans by using spectral analyses where spatial differences in the frequency domain are assessed between overlapping scans after adjustment.(最后这句话实在翻译不来)</p>
<p>此外，还会通过图像之间的交叉相关和SIFT算法来实现更高的精度。在人造模型上的测试用来进行标定，以得到OLA建模精度的一个合理的估计。</p>
<p>[^7]: Strip Adjustment，条带调整，它的主要目标是通过提高相邻条带之间的compatibility，来最大限度第减少激光雷达系统参数中的系统误差的影响</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/logo/GGBond.jpg" alt="Lei Kaiyu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lei Kaiyu</p><p class="is-size-6 is-block">Rubbish Ph. D Candidates</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">13</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/LHospitalLKY" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/LHospitalLKY"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Asteroid-Mission/"><span class="level-start"><span class="level-item">Asteroid Mission</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Navigation/"><span class="level-start"><span class="level-item">Navigation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2023/12/05/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/"><img src="/images/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/preliminary%E8%BD%A8%E9%81%93%E5%8D%95%E7%8B%AC%E5%BB%BA%E6%A8%A1%E7%BB%93%E6%9E%9C.png" alt="Online Shape Modeling of Resident Space Objects Through Implicit Scene Understanding"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-12-05T11:28:15.000Z">2023-12-05</time></p><p class="title"><a href="/2023/12/05/Online-Shape-Modeling-of-Resident-Space-Objects-Through-Implicit-Scene-Understanding/">Online Shape Modeling of Resident Space Objects Through Implicit Scene Understanding</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2023/11/16/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/"><img src="/images/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/cover.png" alt="基于神经网络的光变曲线分类论文阅读(1)：A transfer learning approach to space debris classification using observational light curve data"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-16T02:44:23.000Z">2023-11-16</time></p><p class="title"><a href="/2023/11/16/A-transfer-learning-approach-to-space-debris-classification-using-observational-light-curve-data/">基于神经网络的光变曲线分类论文阅读(1)：A transfer learning approach to space debris classification using observational light curve data</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2023/11/14/RealSense-SDK-rs2-context/"><img src="/images/RealSense-SDK-rs2-context/intel_RealSense_Logo_short.png" alt="RealSense SDK源码分析(1) rs2::context类"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-14T10:41:38.000Z">2023-11-14</time></p><p class="title"><a href="/2023/11/14/RealSense-SDK-rs2-context/">RealSense SDK源码分析(1) rs2::context类</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2023/10/04/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/"><img src="/images/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/DL-Based_SC_relative_navigation.png" alt="Deep learning-based spacecraft relative navigation methods A survey"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-10-04T07:30:36.000Z">2023-10-04</time></p><p class="title"><a href="/2023/10/04/Deep-learning-based-spacecraft-relative-navigation-methods-A-survey/">Deep learning-based spacecraft relative navigation methods A survey</a></p><p class="categories"><a href="/categories/Navigation/">Navigation</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2023/09/22/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/"><img src="/images/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/BennuAsteroid.jpg" alt="Digital terrain mapping by the OSIRIS-REx mission"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-09-22T12:56:46.000Z">2023-09-22</time></p><p class="title"><a href="/2023/09/22/Digital-terrain-mapping-by-the-OSIRIS-REx-mission/">Digital terrain mapping by the OSIRIS-REx mission</a></p><p class="categories"><a href="/categories/Asteroid-Mission/">Asteroid Mission</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Bennu/"><span class="tag">Bennu</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lightcurve/"><span class="tag">Lightcurve</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lightcurve-Classification/"><span class="tag">Lightcurve Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mission-Summary/"><span class="tag">Mission Summary</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Realsense/"><span class="tag">Realsense</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SPC/"><span class="tag">SPC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Space-Object/"><span class="tag">Space Object</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transfer-Learning/"><span class="tag">Transfer Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/View-synthesis-network/"><span class="tag">View-synthesis network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relative-navigation/"><span class="tag">relative navigation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9C%B0%E5%BD%A2%E5%BB%BA%E6%A8%A1/"><span class="tag">地形建模</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><span class="tag">源码阅读</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/logo/GGBond.jpg" alt="Lei Kaiyu&#039; Trash Can" height="28"></a><p class="is-size-7"><span>&copy; 2023 Lei Kaiyu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>